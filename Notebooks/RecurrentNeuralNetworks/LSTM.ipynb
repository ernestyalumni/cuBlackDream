{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfca065c",
   "metadata": {},
   "source": [
    "# Setup Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e674dd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cuBlackDream\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "notebook_directory_parent = Path.cwd().resolve().parent.parent\n",
    "print(notebook_directory_parent)\n",
    "if str(notebook_directory_parent) not in sys.path:\n",
    "    sys.path.append(str(notebook_directory_parent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f7e86e",
   "metadata": {},
   "source": [
    "# Setup to use Python libraries/modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6e87943",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from CuISOX.DataWrangling.Kaggle.DigitRecognizer import ProcessDigitsData\n",
    "from CuISOX.PyTorch.RecurrentNeuralNetworks.LSTM.Examples import (\n",
    "    LSTMWithLinearOutput,\n",
    "    train_LSTMWithLinearOutput_model_on_images,\n",
    "    predict_on_images)\n",
    "from CuISOX.utilities.configure_paths import DataPaths\n",
    "from CuISOX.utilities.DataIO.KagglePaths import KagglePaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19add4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cbfce87",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = DataPaths()\n",
    "kaggle_paths = KagglePaths()\n",
    "kaggle_data_file_paths = kaggle_paths.get_all_data_file_paths()\n",
    "digit_paths = kaggle_data_file_paths[\"DigitRecognizer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3071ce0",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "See [Long-Short Term Memory with Pytorch](https://www.kaggle.com/code/kanncaa1/long-short-term-memory-with-pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81629d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cuBlackDream/Data/Kaggle/DigitRecognizer/digit-recognizer/train.csv\n"
     ]
    }
   ],
   "source": [
    "training_data_paths = DataPaths.get_path_with_substring(digit_paths, \"train\")\n",
    "training_data_path = data_paths.Kaggle() / training_data_paths[0]\n",
    "print(training_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df9350dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_digits_data = ProcessDigitsData()\n",
    "\n",
    "process_digits_data.parse_csv(training_data_path)\n",
    "process_digits_data.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed6962c",
   "metadata": {},
   "source": [
    "## Model Parameters and Sizes, Configuration\n",
    "\n",
    "Batch size, epoch, and iteration\n",
    "\n",
    "Suppose $B \\equiv$ batch size.\n",
    "\n",
    "Take the total number of samples $N$ and divide by $B$ so to get \"number of batches\". Given $N_{\\text{iters}} \\equiv$ total number of iterations, with each iteration doing 1 batch, we can get the total number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a997614",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "# Originally, this value was 6000.\n",
    "n_iters = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87384801",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize one of the images in data set.\n",
    "plt.imshow(X_features_numpy[42].reshape(28, 28))\n",
    "plt.axis(\"off\")\n",
    "plt.title(str(y_targets_numpy[42]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e601c6a",
   "metadata": {},
   "source": [
    "# Run Forward once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92896b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_iterator = enumerate(process_digits_data.training_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "677e3e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28\n",
    "hidden_dim = 100\n",
    "layer_dim = 1\n",
    "output_dim = 10\n",
    "sequence_length = 28\n",
    "# EY: 20230906, we'll use one of the physical dimensions (I'm guessing width) as the \"sequence\" variable. So we\n",
    "# imagine that each row is an input and each successive row makes a sequence of rows of pixels of the image.\n",
    "lstm_with_linear_output = LSTMWithLinearOutput(input_dim, hidden_dim, layer_dim, output_dim, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f2ab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "i, (image_batch, batch_labels) = training_data_iterator.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa565bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_output, example_loss = lstm_with_linear_output.run_on_image_batch(image_batch, batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9260fc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(example_output))\n",
    "print(type(example_loss))\n",
    "print(example_output.size())\n",
    "print(example_loss.size())\n",
    "print(example_output.shape)\n",
    "print(example_loss.shape)\n",
    "print(example_output[0])\n",
    "print(example_output[1])\n",
    "print(example_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2e31a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value, max_indices = torch.max(example_output.data, 1)\n",
    "print(max_value)\n",
    "print(max_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbabe6e",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "518c24ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n"
     ]
    }
   ],
   "source": [
    "number_of_epochs = process_digits_data.calculate_epoch(n_iters, batch_size)\n",
    "# Originally, the value of number_of_epochs was 17 given n_iters = 6000\n",
    "print(number_of_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1efe755b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500, Loss: 2.244734764099121, Accuracy: 19.297618865966797\n",
      "Iteration: 1000, Loss: 0.8516436219215393, Accuracy: 69.70237731933594\n",
      "Iteration: 1500, Loss: 0.4007427990436554, Accuracy: 86.6547622680664\n",
      "Iteration: 2000, Loss: 0.2210036665201187, Accuracy: 92.8452377319336\n",
      "Iteration: 2500, Loss: 0.15652728080749512, Accuracy: 93.94047546386719\n",
      "Iteration: 3000, Loss: 0.16058515012264252, Accuracy: 95.5\n",
      "Iteration: 3500, Loss: 0.11539065092802048, Accuracy: 96.16666412353516\n",
      "Iteration: 4000, Loss: 0.038382187485694885, Accuracy: 96.25\n",
      "Iteration: 4500, Loss: 0.10289300233125687, Accuracy: 96.41666412353516\n",
      "Iteration: 5000, Loss: 0.03403538465499878, Accuracy: 97.14286041259766\n",
      "Iteration: 5500, Loss: 0.09679891914129257, Accuracy: 97.25\n",
      "Iteration: 6000, Loss: 0.08977112919092178, Accuracy: 96.92857360839844\n",
      "Iteration: 6500, Loss: 0.015737101435661316, Accuracy: 97.23809814453125\n",
      "Iteration: 7000, Loss: 0.009149777702987194, Accuracy: 97.41666412353516\n",
      "Iteration: 7500, Loss: 0.044477757066488266, Accuracy: 97.22618865966797\n",
      "Iteration: 8000, Loss: 0.1435568630695343, Accuracy: 97.69047546386719\n",
      "Iteration: 8500, Loss: 0.002778426045551896, Accuracy: 97.83333587646484\n",
      "Iteration: 9000, Loss: 0.031181979924440384, Accuracy: 97.73809814453125\n",
      "Iteration: 9500, Loss: 0.0065725939348340034, Accuracy: 97.92857360839844\n",
      "Iteration: 10000, Loss: 0.019380168989300728, Accuracy: 97.9047622680664\n",
      "Iteration: 10500, Loss: 0.011839766055345535, Accuracy: 98.13095092773438\n",
      "Iteration: 11000, Loss: 0.0142321502789855, Accuracy: 98.13095092773438\n",
      "Iteration: 11500, Loss: 0.023904485628008842, Accuracy: 98.17857360839844\n",
      "Iteration: 12000, Loss: 0.02579360082745552, Accuracy: 98.0952377319336\n",
      "Iteration: 12500, Loss: 0.0443224236369133, Accuracy: 97.9047622680664\n",
      "Iteration: 13000, Loss: 0.01774117350578308, Accuracy: 98.1547622680664\n",
      "Iteration: 13500, Loss: 0.02292744815349579, Accuracy: 97.91666412353516\n",
      "Iteration: 14000, Loss: 0.045229472219944, Accuracy: 98.11904907226562\n",
      "Iteration: 14500, Loss: 0.00909197423607111, Accuracy: 97.63095092773438\n",
      "Iteration: 15000, Loss: 0.007141407113522291, Accuracy: 98.30952453613281\n",
      "Iteration: 15500, Loss: 0.00582043407484889, Accuracy: 98.02381134033203\n",
      "Iteration: 16000, Loss: 0.0052616046741604805, Accuracy: 98.04762268066406\n",
      "Iteration: 16500, Loss: 0.04473119601607323, Accuracy: 98.21428680419922\n",
      "Iteration: 17000, Loss: 0.0010892405407503247, Accuracy: 98.27381134033203\n",
      "Iteration: 17500, Loss: 0.0015196077292785048, Accuracy: 98.23809814453125\n",
      "Iteration: 18000, Loss: 0.004338317085057497, Accuracy: 98.11904907226562\n",
      "Iteration: 18500, Loss: 0.018737787380814552, Accuracy: 98.33333587646484\n",
      "Iteration: 19000, Loss: 0.0013806310016661882, Accuracy: 98.19047546386719\n",
      "Iteration: 19500, Loss: 0.0013656318187713623, Accuracy: 98.29762268066406\n",
      "[tensor(2.2447, grad_fn=<NllLossBackward0>), tensor(0.8516, grad_fn=<NllLossBackward0>), tensor(0.4007, grad_fn=<NllLossBackward0>), tensor(0.2210, grad_fn=<NllLossBackward0>), tensor(0.1565, grad_fn=<NllLossBackward0>), tensor(0.1606, grad_fn=<NllLossBackward0>), tensor(0.1154, grad_fn=<NllLossBackward0>), tensor(0.0384, grad_fn=<NllLossBackward0>), tensor(0.1029, grad_fn=<NllLossBackward0>), tensor(0.0340, grad_fn=<NllLossBackward0>), tensor(0.0968, grad_fn=<NllLossBackward0>), tensor(0.0898, grad_fn=<NllLossBackward0>), tensor(0.0157, grad_fn=<NllLossBackward0>), tensor(0.0091, grad_fn=<NllLossBackward0>), tensor(0.0445, grad_fn=<NllLossBackward0>), tensor(0.1436, grad_fn=<NllLossBackward0>), tensor(0.0028, grad_fn=<NllLossBackward0>), tensor(0.0312, grad_fn=<NllLossBackward0>), tensor(0.0066, grad_fn=<NllLossBackward0>), tensor(0.0194, grad_fn=<NllLossBackward0>), tensor(0.0118, grad_fn=<NllLossBackward0>), tensor(0.0142, grad_fn=<NllLossBackward0>), tensor(0.0239, grad_fn=<NllLossBackward0>), tensor(0.0258, grad_fn=<NllLossBackward0>), tensor(0.0443, grad_fn=<NllLossBackward0>), tensor(0.0177, grad_fn=<NllLossBackward0>), tensor(0.0229, grad_fn=<NllLossBackward0>), tensor(0.0452, grad_fn=<NllLossBackward0>), tensor(0.0091, grad_fn=<NllLossBackward0>), tensor(0.0071, grad_fn=<NllLossBackward0>), tensor(0.0058, grad_fn=<NllLossBackward0>), tensor(0.0053, grad_fn=<NllLossBackward0>), tensor(0.0447, grad_fn=<NllLossBackward0>), tensor(0.0011, grad_fn=<NllLossBackward0>), tensor(0.0015, grad_fn=<NllLossBackward0>), tensor(0.0043, grad_fn=<NllLossBackward0>), tensor(0.0187, grad_fn=<NllLossBackward0>), tensor(0.0014, grad_fn=<NllLossBackward0>), tensor(0.0014, grad_fn=<NllLossBackward0>)] [500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 5500, 6000, 6500, 7000, 7500, 8000, 8500, 9000, 9500, 10000, 10500, 11000, 11500, 12000, 12500, 13000, 13500, 14000, 14500, 15000, 15500, 16000, 16500, 17000, 17500, 18000, 18500, 19000, 19500] [tensor(19.2976), tensor(69.7024), tensor(86.6548), tensor(92.8452), tensor(93.9405), tensor(95.5000), tensor(96.1667), tensor(96.2500), tensor(96.4167), tensor(97.1429), tensor(97.2500), tensor(96.9286), tensor(97.2381), tensor(97.4167), tensor(97.2262), tensor(97.6905), tensor(97.8333), tensor(97.7381), tensor(97.9286), tensor(97.9048), tensor(98.1310), tensor(98.1310), tensor(98.1786), tensor(98.0952), tensor(97.9048), tensor(98.1548), tensor(97.9167), tensor(98.1190), tensor(97.6310), tensor(98.3095), tensor(98.0238), tensor(98.0476), tensor(98.2143), tensor(98.2738), tensor(98.2381), tensor(98.1190), tensor(98.3333), tensor(98.1905), tensor(98.2976)]\n",
      "CPU times: user 18min 37s, sys: 24.9 s, total: 19min 2s\n",
      "Wall time: 3min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lost_list, iteration_list, accuracy_list = train_LSTMWithLinearOutput_model_on_images(\n",
    "    lstm_with_linear_output,\n",
    "    process_digits_data,\n",
    "    number_of_epochs)\n",
    "\n",
    "print(lost_list, iteration_list, accuracy_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216cdda0",
   "metadata": {},
   "source": [
    "# Make predictions with Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e812e4",
   "metadata": {},
   "source": [
    "Get the testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aacff931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DigitRecognizer': [PosixPath('DigitRecognizer/digit-recognizer/sample_submission.csv'), PosixPath('DigitRecognizer/digit-recognizer/test.csv'), PosixPath('DigitRecognizer/digit-recognizer/train.csv')]}\n",
      "/cuBlackDream/Data/Kaggle/DigitRecognizer/digit-recognizer/test.csv\n"
     ]
    }
   ],
   "source": [
    "print(kaggle_data_file_paths)\n",
    "testing_data_paths = DataPaths.get_path_with_substring(digit_paths, \"test\")\n",
    "testing_data_path = data_paths.Kaggle() / testing_data_paths[0]\n",
    "print(testing_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9c85751",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_numpy = ProcessDigitsData.parse_csv_no_split(testing_data_path)\n",
    "test_loader = ProcessDigitsData.load_data_no_split(X_numpy, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c377861f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28000\n",
      "torch.Size([28000, 784])\n"
     ]
    }
   ],
   "source": [
    "print(len(X_numpy))\n",
    "print(X_numpy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1812e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "280\n",
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "digits_predictions = predict_on_images(lstm_with_linear_output, test_loader)\n",
    "print(type(digits_predictions))\n",
    "print(len(digits_predictions))\n",
    "print(digits_predictions[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48473ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miscellaneous work or scratch work.\n",
    "testing_data_iterator = enumerate(test_loader)\n",
    "example_i, example_batch = testing_data_iterator.__next__()\n",
    "print(example_i)\n",
    "print(type(example_batch))\n",
    "print(len(example_batch))\n",
    "print(type(example_batch[0]))\n",
    "print(len(example_batch[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db882fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
