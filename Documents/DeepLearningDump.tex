% file: DeepLearning_dump.tex
% Deep Learning, in unconventional ``grande'' format; fitting a widescreen format
% 
% github        : ernestyalumni
% linkedin      : ernestyalumni 
% wordpress.com : ernestyalumni
%
% This code is open-source, governed by the Creative Common license.  Use of this code is governed by the Caltech Honor Code: ``No member of the Caltech community shall take unfair advantage of any other member of the Caltech community.'' 
% 

\documentclass[10pt]{amsart}
\pdfoutput=1
%\usepackage{mathtools,amssymb,lipsum,caption}
\usepackage{mathtools,amssymb,caption}


\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage[table]{xcolor}
\usepackage{pdfpages}
%\usepackage[version=3]{mhchem}
%\usepackage{mhchem}

\usepackage{tikz}
\usetikzlibrary{matrix,arrows,backgrounds} % background for framed option
\usetikzlibrary{arrows.meta}
\usetikzlibrary{cd}

\usepackage{multicol}

% ----------------------------------------------------------------------------------------
% 20180203

\usetikzlibrary{calc}

% ----------------------------------------------------------------------------------------


\hypersetup{colorlinks=true,citecolor=[rgb]{0,0.4,0}}

\oddsidemargin=15pt
\evensidemargin=5pt
\hoffset-45pt
\voffset-55pt
\topmargin=-4pt
\headsep=5pt
\textwidth=1120pt
\textheight=595pt
\paperwidth=1200pt
\paperheight=700pt
\footskip=40pt


\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{axiom}{Axiom}
%\newtheorem*{main}{Main Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}

\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}

\newenvironment{claim}[1]{\par\noindent\underline{Claim:}\space#1}{}
\newenvironment{claimproof}[1]{\par\noindent\underline{Proof:}\space#1}{\hfill $\blacksquare$}

%This defines a new command \questionhead which takes one argument and
%prints out Question #. with some space.
\newcommand{\questionhead}[1]
{\bigskip\bigskip
	\noindent{\small\bf Question #1.}
	\bigskip}

\newcommand{\problemhead}[1]
{
	\noindent{\small\bf Problem #1.}
}

\newcommand{\exercisehead}[1]
{ \smallskip
	\noindent{\small\bf Exercise #1.}
}

\newcommand{\solutionhead}[1]
{
	\noindent{\small\bf Solution #1.}
}

\title{The Deep Learning Dump}
\author{Ernest Yeung \href{mailto:ernestyalumni@gmail.com}{ernestyalumni@gmail.com}}
\date{19 July 2023}
\keywords{Deep Learning, Deep Neural Networks}
\begin{document}
	
\definecolor{darkgreen}{rgb}{0,0.4,0}
\lstset{language=Python,
	frame=bottomline,
	basicstyle=\scriptsize,
	identifierstyle=\color{blue},
	keywordstyle=\bfseries,
	commentstyle=\color{darkgreen},
	stringstyle=\color{red},
}
%\lstlistoflistings
	
\maketitle
	
From the beginning of 2016, I decided to cease all explicit crowdfunding for any of my materials on physics, math.  I failed to raise \emph{any} funds from previous crowdfunding efforts.  I decided that if I was going to live in \emph{abundance}, I must lose a scarcity attitude.  I am committed to keeping all of my material \textbf{open-sourced}.  I give all my stuff \emph{for free}.   
	
In the beginning of 2017, I received a very generous donation from a reader from Norway who found these notes useful, through \emph{PayPal}.  If you find these notes useful, feel free to donate directly and easily through \href{https://www.paypal.com/cgi-bin/webscr?cmd=_donations&business=ernestsaveschristmas%2bpaypal%40gmail%2ecom&lc=US&item_name=ernestyalumni&currency_code=USD&bn=PP%2dDonationsBF%3abtn_donateCC_LG%2egif%3aNonHosted}{PayPal}. PayPal does charge a fee, so I also have a Venmo, \href{https://account.venmo.com/u/ernestyalumni}{ernestyalumni}, and CashApp (via email \url{mailto:ernestyalumni@gmail.com}).

Otherwise, under the \emph{open-source MIT license}, feel free to copy, edit, paste, make your own versions, share, use as you wish.    

\noindent gmail        : ernestyalumni \\
linkedin     : ernestyalumni \\
twitter      : ernestyalumni \\

		
\begin{multicols*}{2}
		
\setcounter{tocdepth}{1}
\tableofcontents
		
\begin{abstract}
Everything Deep Learning, Deep Neural Networks
\end{abstract}
		
\part{Deep Neural Networks}

\section{Gaussian Processes}
Yang (2021) for Tensor Programs I\cite{Yang2021}

\part{Transformer Networks}

Including \emph{Attention}

\section{Transformers}

\subsection{Input}

See Turner (2023) \cite{Turn2023}.

Let input data s.t. sequence of $N$ $\mathbf{x}^{(0)}_n$ of dim. $D$, $n = 0,1, \dots N-1$, $\mathbf{x}_n^{(0)} \in F^D$, where $F$ is some field (i.e. some data type such as float, double, etc.). \\

Let matrix $X^{(0)} \in F^{D\times N}$ or $\text{Mat}_F(D, N)$, a sequence of $N$ arrays of dim. $D$ collected into a matrix. \\

Let $M \in 0,1,\dots $ i.e. $M \in \mathbb{Z}^+$. \\

The goal is to map $X^{(0)}$ to $X^{(M)} \in \text{Mat}_F(D,N)$ i.e. $X^{(M)}$ of size $D\times M$ s.t. since $x_n = X_{;n}^{(M)}$ is a vector of features representing the sequence at location of $n$ in the sequence.

\subsection{Attention $A^{(m)}$ }

Consider output vector at location $n$, $\mathbf{y}_n^{(m)}$, where 
\begin{equation}
\mathbf{y}_n^{(m)} = \mathbf{x}^{(m -1)}_{n'} A^{(m)}_{n' n}, \quad \, n' = 0,1, \dots N-1
\end{equation} 
(Eq. (1) of Turner (2023) \cite{Turn2023}), where $A^{(m)}_{n' n} = A^{(m)}$ is called the attention matrix, $A^{(m)} \in \text{Mat}_F(N, N)$ and normalizes over its columns:
\begin{equation}
\sum_{n' = 1}^N A^{(m)}_{n' n}  =1
\end{equation}

\subsection{Projection of Q, K, V, queries, keys, and values}

Recall an input $\mathbf{x}_n = X^{(M)}_{;n} \in F^D$. Recall the linear transform resulting so-called queries or query vectors:
\[
\mathbf{q}^{(m)}_{h; n} = U^{(m)}_{q;h} \mathbf{x}^{(m-1)}_n \in F^K, \quad \, U^{(m)}_{q; h} \in \text{Mat}_F(K, D)
\]
where $h=0,1,\dots H-1$ with $H$ heads in Turner's notation (Turner (2023)\cite{Turn2023}). Compare this with NVIDIA's notation, \cite{NSAtn2023}, $i=0,1,\dots \texttt{nHeads} - 1$, so that $H \equiv \texttt{nHeads}$.\\

Generalize $K$ in dimensiosn $k\times D$ of $U^{(m)}_{q;h}$ to \texttt{qSize} $\equiv D_q$, i.e.
\[
\mathbf{q} \equiv \mathbf{q}^{(m)}_{h ; n} = U^{(m)}_{q;h} \mathbf{x}_n^{(m-1)} \in F^{D_q}, \quad \,  U^{(m)}_{q;h} \in \text{Mat}_F(D_q, D)
\] 

See \href{https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnSetAttnDescriptor}{7.2.45. \texttt{cudnnSetAttnDescriptor}}

\end{multicols*}

\begin{thebibliography}{9}

\bibitem{Yang2021} 
Greg Yang. "Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes." \href{https://arxiv.org/pdf/1910.12478.pdf}{arXiv:1910.12478v3} 8 May 2021

\bibitem{Turn2023}
Richard E. Turner. "An Introduction to Transformers". \href{https://arxiv.org/pdf/2304.10557.pdf}{arXiv:2304.10557v3 cs.LG 4 Jul 2023}

\bibitem{NSAtn2023}
NVIDIA. "7.2.45 \texttt{cudnnSetAttnDescriptor}". cuDNN API Documentation. \href{https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnSetAttnDescriptor}{7.2.45. \texttt{cudnnSetAttnDescriptor}}

\end{thebibliography}
\end{document}
